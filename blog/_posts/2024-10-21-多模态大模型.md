---
layout: post
title: 多模态大模型
truncated_preview: true
excerpt_separator: <!--more--> 


---

<div class="message">
    多模态大模型是一个讲录音，图片，文字结合在一起的模型，端到端的训练，可能图片采用的是clip，语音采用的是whisper，大模型这一段有各种各样的，但是万变不离其宗，但是我们想要搞懂，多模态大模型是如何训练的，是一个非常有意思的事情，包括多模态理论上是可以做什么的？或者说多模态大模型能力的边界是什么？我决定从下手，当然这是一个很好的一个example,篇幅并不是很大，并且母语为中文来写的英文论文，中国人比较看得懂（笑。其次最近也有一些其他的工作陆续推出，比如，之前omini1的端到端是没有图片的识别，现在也支持了更多的模态，并且一些图片模态的也开始陆续支持声音的这一向量。
</div>    
<!--more-->

[Qwen2vl](https://arxiv.org/pdf/2409.12191)  [mini-omni2](https://arxiv.org/pdf/2410.11190)
    对于从Qwen2-vl，我想搞清楚三个问题：
    1. 多模态模型是怎么训练的
    2. 为什么图像能做OCR？token化图像，为什么不会被撕裂掉？
    3. 怎么做function call?
    4. 有没有更快的方法可以和大模型进行交互？（开放性问题）
    
1. 多模态模型是怎么训练的？

    首先训练数据都用到了ChatML（openai）提出的，这个就是现在数据为什么都使用一个Chat template
一个是训练对话能力。
一个训练vedio grounding能力，也就是所谓的是视觉基础能力，里面训练数据会被插入被标准化到1到1000的数值，

    我查看了去年比较早起的LLava工作，讲的是如何讲视觉信息融入大模型的语义，通过不断去学习vit投射到模型输入的同一语义空间中，
    语义上的对齐，并且制作了多种的语言模板，大概结构是图片+图片描述+针对图片上的问答，大概有100K左右的instruction following data。
    其次这种instrction的问答都是用当时的刚宣布自己支持图片模态的GPT4制造出来的数据。
    
2.  然后出来了一个gpt40，拥有实时和人类对话的模型。

    对于Mini-omni，我想搞清楚，他和视觉模型有没有一些工程上的不一样？
    
    不过这个的数据，是通过gpt40制造出来的数据。

